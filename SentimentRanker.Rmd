---
title: "Using Sentiment Analysis to Improve YouTube Recommended Video Rankings"
author: "Mark Richards"
date: "5/10/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
header-includes: \usepackage{float}
urlcolor: blue
---
[GitHub](https://github.com/dteck/YTSentiment) Repository

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=TRUE)
```

## Introduction

This report will cover a method for improving YouTube recommendations based on natural language processing and sentiment analysis. The goal is to improve the ranking of recommended videos by evaluating the emotions that a video evokes in those that comment on it against the emotions expressed in the comments of a list of recommended videos. It is not meant to be a replacement for the deep neural network that Google uses to build its recommendations. It is simply meant to demonstrate the a potential for incorporating sentiment analysis as part of the ranking system for serving recommendations to users.   
This project requires some background knowledge of YouTube and how it generates its recommendations. In 2016 Google released a paper titled ["Deep Neural Networks for YouTube Recommendations"](https://ai.google/research/pubs/pub45530) where it outlines its use of two deep neural nets to build and rank recommendations. This paper covers the processes they use in great detail unfortunately however it reads like an academic journal article.  
In 2018 Moin Nadeem a then Junior at the Massachusetts Institute of Technology (MIT) wrote a more human friendly overview of the topics covered in the Google paper for the website Towards Data Science titled simply ["How YouTube Recommends Videos"](https://towardsdatascience.com/how-youtube-recommends-videos-b6e003a5ab2f).    
A brief overview is that Google uses two deep neural networks to create suggestions. The first looks at things like videos previously watched, search history, and demographics to narrow down the pool of videos to be ranked. The second neural net incorporates additional features such as preview image and peer interest and uses that to order the list of recommendations (Nadeem 2018). The goal of these networks is to maximize watch time. A diagram of this process is included below.

```{r YTdiag, echo=FALSE, out.width = '70%'}
knitr::include_graphics("YouTube_DNN_Diag.png")
print("Diagram of YouTube Deep Neural Networks (Nadeem 2018)")
```



At the end of April 2019 news reports such as ["Alphabet had more than $70 billion in market cap wiped out, and it says YouTube is one of the problems"](https://www.cnbc.com/2019/04/30/youtube-algorithm-changes-negatively-impact-google-ad-revenue.html) began to appear. These articles blame a dip in YouTube engagement and ad revenue for the drop in market value. They also indicate that YouTube’s recent efforts to curb fake news and conspiracy theories may be to blame for the drop in engagement. It may be that adding sentiment analysis could help improve engagement again by connecting users with content that matches emotional tones.  

##Analysis
**Note: ** To properly run the code in this analysis an API key is required. Instructions on how to acquire an API key can be found here: ["YouTube Data API Overview"](https://developers.google.com/youtube/v3/getting-started). This file will not be provided on the [GitHub](https://github.com/dteck/YTSentiment) Repository for this paper. A copy of this script for those who wish to set up their own API key is available on my GitHub as [SentimentRanker_wKey.R]( https://raw.githubusercontent.com/dteck/YTSentiment/master/SentimentRanker_wKey.R) All of the API calls rely on the TubeR package to handle the Oauth token.   
There is also an issue of reproducibility. The suggestions served to me in this report will almost certainly not be reproducible by anyone else. They are generated by the aforementioned deep neural nets and as such are tailored to what YouTube believes would best maximize my watch time. To counter this issue for the report I have created RDS files of any data that needs to be pulled from YouTube. The code for this report and the accompanying R script will call these RDS files rather than making the API calls. To examine how the API calls work see the script provided for those with keys.
 
Before running the script make sure you have the following RDS files in your working directory.

1.  [RelatedVids.rds](https://github.com/dteck/YTSentiment/raw/master/RelatedVids.rds)
2.	[BaseVid.rds](https://github.com/dteck/YTSentiment/raw/master/baseVid.rds)
3.	[BaseDetail.rds](https://github.com/dteck/YTSentiment/raw/master/BaseDetail.rds)
4.	[RelatedComm.rds](https://github.com/dteck/YTSentiment/raw/master/RelatedComm.rds)

There are several packages we will require to start our analysis. This code will install and load them.
```{r Libraries,  warning=FALSE, message=FALSE}
if (!require(tuber)) install.packages('tuber')
if (!require(syuzhet)) install.packages('syuzhet')
if (!require(ggplot2)) install.packages('ggplot2')
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(knitr)) install.packages('knitr')
if (!require(kableExtra)) install.packages('kableExtra')
if (!require(stringr)) install.packages('stringr')
library(tuber)
library(syuzhet)
library(ggplot2)
library(tidyverse)
library(knitr)
library(kableExtra)
library(stringr)
```


Once the packages are installed and loaded the script would check to make sure you have an API key installed. If you did not it would display a URL where you could learn how to set one up. If it did find a key RDS it would load the file and setup the Oauth token. It would then direct you to a website to authorize the app to access your account. Since you are not expected to set up an API key this code is only shown as an example. 
```{r APIKeyLoad, eval=FALSE}
APIKey<-file_test("-f","APIKey.rds") #test if apikey exists
if (APIKey == FALSE) { #run code block if apikey does not exist
  print("No youtube API Key")
  print("See https://developers.google.com/youtube/v3/getting-started")
  print("expects key as RDS data.frame in form 
        APIKey<-data.frame(app_id='Your App ID Here', 
        app_secret = 'Your App Secret Here')")
  rm(APIKey) #removes file test
}else { #run code block if api key exists
  APIKey<-readRDS("APIKey.rds") #read API key
  yt_oauth(app_id=APIKey$app_id, 
           app_secret = APIKey$app_secret, token = "") #load key to memory
}
```


Now that the API key is glossed over, it’s time to pick a video that we want recommendations for. For this report I will be using a video from the cooking channel [Binging With Babish](https://www.youtube.com/channel/UCJHA_jMfCvEnv-3kRjTCQXw) where he attempts to recreate [Jake's Perfect Sandwich from Adventure Time](https://www.youtube.com/watch?v=HsxBw6ls7Z0). The following code is where the URL for the video we like is entered. This is referred to as the “Base” video from this point on.
```{r BaseVideoLink}
Baselink<-"https://www.youtube.com/watch?v=HsxBw6ls7Z0" #link to a liked youtube video
```

Once the base video URL is entered, we need to separate out the Video ID. This is the string of alphanumeric characters after the "=" in the URL. A regular expression is used to select everything past the "=" sign and save it for use with the API calls.
```{r BaseVideoID}
Baselink<-str_match(Baselink,"[^=]+$") #regex to extract video ID
```

With the Video ID isolated we can begin making calls to the YouTube API and pulling data down for analysis. Again, the code is shown as an example but the [RelatedVids.rds](https://github.com/dteck/YTSentiment/raw/master/RelatedVids.rds) will be loaded with the results of the API calls.
```{r BaseVideoInfo, warning=FALSE, message=FALSE, results='hide', eval=FALSE}
RelatedVids<- get_related_videos(video_id =Baselink, max_results = 11) #pull recomended
```
```{r BaseVideoRDS, warning=FALSE, message=FALSE, results='hide'}
RelatedVids<- readRDS("RelatedVids.rds") #Load RDS with API results
```

This call returns a list of videos with 17 variables. To view the list we will only display the two most releveant variables Video ID and Title. We will save this list for later.
```{r BaseVideoTable}
RelatedTable<-RelatedVids[,c("rel_video_id","title")]
kable(RelatedTable) %>% 
  kable_styling(latex_options="scale_down")
```


With the list of recommendations stored it’s time to start analyzing our Base video. The API lets us pull all of the comments for a video with the function get_all_comments(). We also want to use the function get_video_details(). Both of these functions require an API key so I will show the code used and then have the script load RDS files with the results. The code will also extract the title from the Video Details.
```{r BaseVideoDetails, warning=FALSE, message=FALSE, results='hide', eval=FALSE}
baseVid<-get_all_comments(video_id = Baselink) #get the comments from that video
BaseDetail<-get_video_details(video_id = Baselink)#pull details of the base video
BaseTitle<-BaseDetail$items[[1]]$snippet$title #extract the title of the base video
```
```{r BaseVideoDetailsRDS, warning=FALSE, message=FALSE, results='hide'}
baseVid<-readRDS("baseVid.rds") #Load RDS with API results
BaseDetail<-readRDS("BaseDetail.rds") #Load RDS with API results
BaseTitle<-BaseDetail$items[[1]]$snippet$title #extract the title of the base video
```

The API call to get all comments tells us that the Base video has ```r  length(baseVid$id) ``` comments. The Video Details include a lot of information about things like the video description, thumbnail urls, video tags, title, etc. Again we are mainly focused on extracting the title so we can display it with our graphs later.  
A quick view of the comments data frame shows us things like who made the comment, what the comment said, and how many people liked the comment. We can take a look at the top rated comments, after removing the Unicode characters since LaTex does not like them.


```{r BaseCommentTable}

BaseCommentTable<-baseVid[,c("authorDisplayName" ,"textOriginal","likeCount")]
BaseCommentTable$textOriginal<-iconv(str_trunc(BaseCommentTable$textOriginal,80,"right"),
                                     to="ASCII")
BaseCommentTable$authorDisplayName<-iconv(BaseCommentTable$authorDisplayName, to="ASCII")
BaseCommentTable$likeCount<-as.numeric(BaseCommentTable$likeCount)
BaseCommentTable<-BaseCommentTable[order(BaseCommentTable$likeCount, decreasing = TRUE),]
kable(head(BaseCommentTable,10)) %>%  kable_styling(latex_options="scale_down")
```


With the comments for the base video ready to go we can make sure there are no weird characters by converting them to UTF-8 and then feed them to our sentiment analysis engine. In this case we will be using the [syuzhet](https://cran.r-project.org/web/packages/syuzhet/index.html) package. and its nrc_sentiment() function. This function loads a dictionary of words that have been tagged with a set of emotions and the valance (positive or negative). The function then matches the words in our comments with words in the dictionary and tells us what emotions are expressed and if the valance is positive or negative. The NRC Word-Emotion Association Lexicon was built by Saif Mohammad and the National Research Council of Canada. More information about it can be found here: [NRC Emotion Lexicon](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). We can look at the first few rows to see what the results are.

```{r SanitarySentiment, cache=TRUE}
#sanitize the text by converting to utf-8
baseVid$sanitary<-iconv(baseVid$textOriginal, to="UTF-8")
#get emotional levels from each comment
BaseEmo<-get_nrc_sentiment(baseVid$sanitary)
kable(head(BaseEmo,10)) %>%  kable_styling(latex_options="scale_down")
```

These results are interesting but not very useful on their own. We are more interested in the emotions people are expressing overall. To find that we can take the average of each of these columns and get a kind of emotional profile for how the audience reacts and interacts.
```{r BaseEmotionalAverage, cache=TRUE}
#create averages of sentiment and emotion for base video
Baseavg<-apply(BaseEmo, 2, mean)
#convert averages to data frame for plotting
Baseavg<-data.frame(name=Baseavg)
kable(Baseavg)
```

We can now take this table of the average emotions expressed in the comments for the base video and turn it into a more intuitive representation with ggplot.
```{r BaseEmotionalPlot, cache=TRUE}
#plot the emotional sentiment averages
Avg<-ggplot(Baseavg, aes_(x=row.names(Baseavg), y=Baseavg$name,
fill=row.names(Baseavg)))+geom_bar(stat="identity")+theme(legend.position="none",
                       axis.title.x=element_blank(),axis.title.y=element_blank())
Avg+ggtitle(BaseTitle,paste("Video ID: ",baseVid$videoId[1]))
```

Here we can see that the comments are overwhelmingly positive. While Anticipation, Joy, Negativity, and Trust were all about equally represented. This graph shows us the general level of different emotions expressed by people who commented on the video. This is the emotional profile that we will be attempting to match across our set of recommended videos. Depending on how close they come to this exact profile is how we will rank them.   
The next step is to use the API to pull the comments for our list of recommended videos. The following code would build a list of data frames that contain all of the comments for the recommended videos. For this report I will show the code and then load the results from an RDS file. I will also display the number of comments on each of the related videos. 

```{r RelatedComments, cache=TRUE, eval=FALSE}
seq<-1:length(RelatedVids$rel_video_id) #set iteration length
RelatedComm<-list() #initalize a list for the related video comments
for (n in seq){ #get comments,sanitize by converting to UTF-8 format
  RelatedComm[[n]]<-get_all_comments(video_id = as.character(RelatedVids$rel_video_id[n]))
  RelatedComm[[n]]$sanitary<-iconv(RelatedComm[[n]]$textOriginal, to="UTF-8")
  RelatedComm[[n]]$likeCount<-as.numeric(RelatedComm[[n]]$likeCount)
  RelatedTable$Comments[n]<-length(RelatedComm[[n]]$id)
}
rm(n,seq) #remove n and sequence used to itterate over list
```
```{r RelatedCommentsRDS, cache=TRUE}
RelatedComm<-readRDS("RelatedComm.rds") #Load RDS with API results
seq<-1:length(RelatedVids$rel_video_id)
for (n in seq){
RelatedTable$Comments[n]<-length(RelatedComm[[n]]$id) #calc comments
}
rm(n,seq)
kable(RelatedTable) %>% #display table for RMD
  kable_styling(latex_options="scale_down")
```

Here the process repeats a bit. The code will go through and calculate the emotions for all of the comments on the related videos. **Warning:** this can take several minutes. 
```{r RelatedSentimentsAll, cache=TRUE}
seq<-1:length(RelatedVids$rel_video_id) #set iteration length
RelatedEmo<-list()
for (n in seq){ #itterate over all comments and get emotional sentiment
  RelatedEmo[[n]]<-get_nrc_sentiment(RelatedComm[[n]]$sanitary)
}
rm(n,seq) 
```

This next bit of code will take the emotions and calculate the averages for each video. Similar to how we did it for the Base video.
```{r RelatedAvgAll, cache=TRUE}
seq<-1:length(RelatedEmo) #set iteration length
RelatedEmoAvg<-list() 
for (n in seq){ #calulate averages of emotional senitment
  RelatedEmoAvg[[n]]<-apply(RelatedEmo[[n]][1:10], 2, mean)
}
rm(n,seq)
```


At this point there is a bit of data reshaping that needs to be done. The emotional averages for each video are converted from a list to a data frame. We also add the Video ID in so we can keep track of what emotional profile belongs to which video. To see what this looks like after a table will be included.
```{r RelatedEmoTranspose, cache=TRUE}
RelatedEmoAvg<-data.frame(RelatedEmoAvg) #reshape the dataframe
RelatedEmoAvg<-data.frame(t(RelatedEmoAvg)) #transpose the df
rownames(RelatedEmoAvg)<-RelatedVids$rel_video_id #set row names
kable(RelatedEmoAvg) %>% #show table
  kable_styling(latex_options="scale_down") #fit to page
```

After a lot of calculations, we now have the emotional profiles for each of the related videos. Now we have to create a way to rank them. We know that we want to match the emotional profile of the Base video. So we will start by calculating the absolute difference of the emotions for each of the related videos from the Base video.
```{r AbsoluteEmo, cache=TRUE}
Baseavg<-data.frame(t(Baseavg)) #transpose the baseavg df
AbsoluteEmo<-data.frame() #initialize ABS difference dataframe
seq<-1:length(RelatedEmo) 
for (n in seq){ #calculate difference between base emotions and related video emotions
  AbsoluteEmoTemp<-abs(Baseavg[1,]-RelatedEmoAvg[n,])
  AbsoluteEmo<-rbind(AbsoluteEmo,AbsoluteEmoTemp)
}
rm(n,seq,AbsoluteEmoTemp) 
rownames(AbsoluteEmo)<-RelatedVids$rel_video_id #set row names
kable(AbsoluteEmo) %>% 
  kable_styling(latex_options="scale_down")
```


The first thing we do is transpose the data frame. This makes it so that each video is a column and each row is an emotion. Then we use the apply() function on each of the columns to rank the emotions. The smaller the number the closer it was to the base video. The larger the farther away from the base video it was.
```{r BuildRanks, cache=TRUE}
AbsoluteEmo<-data.frame(t(AbsoluteEmo)) #transpose dataframe
AbsoluteEmo<-apply(AbsoluteEmo,2,rank) #get rank of abs diff for each emotion by video
kable(AbsoluteEmo) %>% 
  kable_styling(latex_options="scale_down")
```
From this table we can see that for the first video fear was the emotion that most closely matched the base video. While positivity was the farthest away from the base video.  

With the emotions for each of the related video ranked by how close they were to the base we want to transpose the data back to its original shape.
```{r BuildRanks2, cache=TRUE}
AbsoluteEmo<-t(AbsoluteEmo) #transose dataframe
kable(AbsoluteEmo) %>% 
  kable_styling(latex_options="scale_down")
```
Here we again see that for the first video fear was the emotion that most closely matched the base video. While positivity was the farthest away from the base video.  

With the data back in this format we want to use apply() to rank down the columns again. This will tell us for each emotion, what is the order for how close each video was to the Base video relative to the overall emotional profile for that video. 
```{r BuildRanks3, cache=TRUE}
AbsoluteEmo<-apply(AbsoluteEmo,2,rank) #rank columns to get rank of each emotion for all 
AbsoluteEmo<-data.frame(AbsoluteEmo) #convert back to dataframe
kable(AbsoluteEmo) %>% 
  kable_styling(latex_options="scale_down")
```


Now we want to sum up all of the rankings for a row with the code below.
```{r BuildRanks4, cache=TRUE}
AbsoluteEmo$sum<-rowSums(AbsoluteEmo) #add sum of rows
kable(t(AbsoluteEmo$sum))
```

With the row sums in place we want to add a few more items like the video titles and build a link to each of the videos.
```{r BuildRanks5, cache=TRUE}
AbsoluteEmo$title<-RelatedVids$title #add titles to emo df
AbsoluteEmo$link<-paste("https://www.youtube.com/watch?v=",
RelatedVids$rel_video_id, sep="") #build links to videos
```

Now we create a ranked list of the videos using the sum of the rankings. We make sure to break any ties that may occur by having the script select the first entry with a duplicate row sum to be higher rank.
```{r RankedList, cache=TRUE}
RankedList<-data.frame(rank=rank(AbsoluteEmo$sum, ties.method ="first"),
title=AbsoluteEmo$title,link=AbsoluteEmo$link) 
#generate a list of recommended videos and rank of best emotional fit
kable(RankedList) %>% 
  kable_styling(latex_options="scale_down")
```

From this ranked list we may want to see how close the top recommendation is to our Base video. The following code plots the best ranked videos emotional profile overtop of the Base video emotional profile.
```{r TopReccPlot, cache=TRUE}
TopRank<-which.min(RankedList$rank)# get the topranked video index value
ClosestMatch<-RelatedEmoAvg[which.min(RankedList$rank),] 
#pull emotional avg values from closest match
ClosestMatch<-data.frame(t(ClosestMatch)) #transpose df
colnames(ClosestMatch)<-"1" #set column name to set value for graphing
Recc<-Avg+geom_point(data=ClosestMatch,aes(shape=18, size=5, y=ClosestMatch$'1', 
x=colnames(Baseavg)))+scale_shape_identity()+ggtitle(paste(BaseTitle,"-",
baseVid$videoId[1]),paste("Reccomended Video: ",
RelatedVids$title[TopRank]," - ",
RelatedVids$rel_video_id[TopRank]))
#add to ggplot to show how close best match video is
Recc
```

From this graph we can see that the top recommendation is quite close in several of the emotional categories.  
As a final visualization we plot all of the videos to see the spread of emotion across all of the suggestions.

```{r ReccPlotAll, cache=TRUE}
RelatedEmoAvgTrans<-data.frame(t(RelatedEmoAvg)) # reshape df for graphing
seq<-c(1:length(RelatedVids$rel_video_id)) #set number of itterations
for (n in seq){ #add data points for each video to the graph to show spread.
  Recc<-Recc+geom_point(data=RelatedEmoAvgTrans,aes_(y=RelatedEmoAvgTrans[,n], alpha=0.2))
}
rm(n,seq) 
Recc #display final graph
```

After all of this we save the list to a CSV in the working directory. 
```{r SaveList}
write.csv(RankedList,"RankedList.csv", row.names = FALSE) #output ranked list to csv
```


##Results

In the table below we can see that we provided a different order of recommended rankings than the one provided by YouTube (first column). This implies that there is potential for improvement in the suggestion system by matching emotional tones expressed by viewers. 

```{r DisplayList, cache=TRUE}
kable(RankedList[order(RankedList$rank),]) %>% 
  kable_styling(latex_options="scale_down")
```

To quantify this we can calculate the difference in video ranking positions. 
```{r ListDelta, cache=TRUE}
RankedList$ind<-c(1:10)
RankDelta<-sum(abs(RankedList$ind-RankedList$rank))
```
Here we see that the total number of ranking positions shifted is ```r  RankDelta ```.

##Conclusion
We have found that there is room to improve the recommendations for videos served by YouTube with the addition of natural language processing to match emotional tones across suggestions. This also raises the possibility of using these methods to build sentiment profiles of users. One path would be to combine the sentiments of every comment left by a user. As well as building a profile for every video they like. From this YouTube could infer what emotions a user is drawn to and feed that back to them to increase their engagement and watch time.  
There is a consideration that should be made for performance. This paper is a proof of concept and would not be the most efficient way to implement this type of ranking. It would make more sense for YouTube to build its own lexicon that is specific to its comments. This would allow them to get more accurate results regarding expression of emotions and valance of comments.  
They should also consider running the sentiment analysis across all comments currently available and then going forward when a user submits a comment or edits one it does the sentiment processing at submission time. The results of these comments and video averages can be stored for quick processing when building and ranking suggestions.  
As a further step when auto translation becomes more robust sentiment analysis could be run across the video transcripts to build profiles of a user’s preferred emotional content and better match recommendations based on this.  
All of these steps may help improve YouTubes core metric of watch time but proper A/B testing which is outside of the scope of this paper would be required to estimate the true impact. 
